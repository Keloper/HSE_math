{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_Europe2020/master/images/linear_dogs.jpg\" width=\"800\"> \n",
    "</center>  \n",
    "\n",
    "# Yet another math for DS course: домашнее задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ФИО:**\n",
    "\n",
    "**Забавный факт о себе:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Жёсткий дедлайн:__ 23:59MSK 02.11.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формат сдачи\n",
    "\n",
    "Сам ноутбук называйте в формате hw-04-USERNAME.ipynb, где USERNAME — ваши фамилия и имя. Файл надо будет сдать в anytask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача №1: SVD (5 баллов)\n",
    "\n",
    "Напомним, что сингулярным разложением (SVD, Singular value decomposition) матрицы $A$ размера $m\\times n$ называется представление\n",
    "\n",
    "$$A = U\\Sigma V^T,$$\n",
    "\n",
    "где $U$ — ортогональная матрица размера $m\\times m$, $V$ — ортогональная матрица размера $n\\times n$, $\\Sigma = \\mathrm{diag}(\\sigma_1,\\sigma_2,\\sigma_3,\\ldots)$ — диагональная матрица размера $m\\times n$, в которой $\\sigma_1\\geqslant\\sigma_2\\geqslant\\ldots\\geqslant0$.\n",
    "\n",
    "На самом деле требование, чтобы матрицы $U$ и $V$ были квадратными, избыточно. *Усечённым сингулярным разложением* мы будем называть разложение\n",
    "\n",
    "$$A = U\\Sigma V^T,$$\n",
    "\n",
    "где $U$ и $V$ — матрицы с ортонормированными столбцами размеров $m\\times k$ и $n \\times k$ соответственно, $\\Sigma$ — диагональная матрица размера $k\\times k$, где $k = \\min(m,n)$. Далее мы будем работать исключительно с усечённым разложением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сжатие данных с помощью SVD = построение низкорангового приближения\n",
    "\n",
    "Введём *норму Фробениуса* матрицы как\n",
    "\n",
    "$$||A||_{frob} = \\sqrt{\\mathrm{tr}{A^TA}} = \\sqrt{\\sum\\limits_{i,j}a^2_{i,j}}$$\n",
    "\n",
    "Иными словами, это обычное евклидово расстояние на пространстве, которое получается, если все матрицы вытянуть в длинные векторы.\n",
    "\n",
    "Зададимся вопросом: как найти матрицу $A_{r}$ ранга $r$, наименее отличающуюся от $A$ по норме Фробениуса (то есть для которой норма разности $||A - A_{r}||_{frob}$ минимальна). Оказывается, это можно сделать с помощью сингулярного разложения:\n",
    "\n",
    "**Теорема.** Пусть $\\Sigma_{r}$ — это матрица, полученная из $\\Sigma$ заменой диагональных элементов $\\sigma_{i}$ ($i > r$) нулями, тогда $A_{r} = U\\Sigma_{r}V^T$.\n",
    "\n",
    "Это можно переписать и в более экономичном виде. Если\n",
    "\n",
    "$$A = \\underbrace{\n",
    "\\begin{pmatrix}\n",
    "u_{11} & \\ldots & u_{1k}\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "u_{m1} & \\ldots & u_{mk}\n",
    "\\end{pmatrix}}_{=U}\\cdot\\underbrace{{\n",
    "\\begin{pmatrix}\n",
    "\\sigma_{1} & &\\\\\n",
    " & \\sigma_{2} & \\\\\n",
    " & & \\ddots\n",
    "\\end{pmatrix}}\n",
    "}_{=\\Sigma}\\cdot \\underbrace{\n",
    "\\begin{pmatrix}\n",
    "v_{11} & \\ldots & v_{n1}\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "v_{1k} & \\ldots & v_{nk}\n",
    "\\end{pmatrix}}_{=V^T}$$\n",
    "\n",
    "то\n",
    "\n",
    "$$A_{r} = \\underbrace{\n",
    "\\begin{pmatrix}\n",
    "u_{11} & \\ldots & u_{1r}\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "u_{m1} & \\ldots & u_{mr}\n",
    "\\end{pmatrix}}_{=U_r}\\cdot\\underbrace{{\n",
    "\\begin{pmatrix}\n",
    "\\sigma_{1} & &\\\\\n",
    " & \\ddots & \\\\\n",
    " & & \\sigma_{r}\n",
    "\\end{pmatrix}}\n",
    "}_{=\\Sigma_r}\\cdot \\underbrace{\n",
    "\\begin{pmatrix}\n",
    "v_{11} & \\ldots & v_{n1}\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "v_{1r} & \\ldots & v_{nr}\n",
    "\\end{pmatrix}}_{=V^T_r}$$\n",
    "\n",
    "При этом\n",
    "\n",
    "$$||A - A_{r}||_{frob} = \\sqrt{\\sum\\limits_{i\\geqslant r+1} \\sigma_{i}^2}$$\n",
    "\n",
    "Если сингулярные значения матрицы падают достаточно быстро (а в реальных задачах часто бывает именно так), то норма разности будет малой при сравнительно небольшом значении $r$.\n",
    "\n",
    "На хранение исходной матрицы нам требовалось $m\\times n$ памяти. Теперь же, если мы будем хранить отдельно $U_r$, $V_r$ и диагональные элементы $\\Sigma_r$, затраты памяти составят $mr + nr + r = r(m + n + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разложение на компоненты ранга 1**\n",
    "\n",
    "Обозначим через $u_1,\\ldots, u_k$ столбцы матрицы $U$, а через $v_1, \\ldots, v_k$ столбцы матрицы $V$. Тогда имеет место равенство\n",
    "\n",
    "$$A = u_1\\sigma_{1}v_1^T + u_2\\sigma_{2}v_2^T + u_3\\sigma_{3}v_3^T + \\ldots$$\n",
    "\n",
    "Матрицу $u_k\\sigma_{k}v_k^T = \\sigma_{k}u_kv_k^T$ мы будем называть $k$-й компонентой ранга 1 матрицы $A$. Отметим, что слагаемые в этой сумме ортогональны относительно скалярного произведения $(X, Y) = \\mathrm{tr}(X^TY)$ (порождающего норму Фробениуса).\n",
    "\n",
    "Как нетрудно заметить,\n",
    "\n",
    "$$A_{r} = \\sigma_{1}u_1v_1^T + \\sigma_{2}u_2v_2^T + \\ldots + \\sigma_{r}u_rv_r^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Геометрический смысл SVD**\n",
    "\n",
    "Допустим, что у нас есть выборка $x_1,\\ldots,x_m\\in\\mathbb{R}^n$. Запишем её в матрицу объекты-признаки\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "x_{11} & \\ldots & x_{1n}\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "x_{m1} & \\ldots & x_{mn}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "(строки — объекты, столбцы — признаки) и сделаем SVD-разложение: $X = U\\Sigma V^T$. Его можно интерпретировать следующим образом:\n",
    "\n",
    "$$X = U\\Sigma\\cdot V^T,$$\n",
    "\n",
    "где $U\\Sigma$ — это матрица объекты-признаки для тех же объектов, но в новых признаках, полученных из исходных с помощью линейного преобразования $V$ (напоминаем, что умножение на матрицу справа соответствует преобразованию столбцов). Попробуем понять, чем замечательны эти признаки.\n",
    "\n",
    "Рассмотрим матрицу $X^TX = V\\Sigma^2V^T$. Легко видеть, что это матрица Грама системы столбцов матрицы $X$; иными словами, в ней записаны скалярные произведения векторов различных признаков. Из лекций вы знаете, что $\\sigma_1^2$, квадрат первого сингулярного числа, это наибольшее собственное значение матрицы $X^TX$, а $v_1$, первый столбец матрицы $V$, — это соответствующий собственный вектор. Можно показать, что\n",
    "\n",
    "$$\\sigma_1 = \\mathrm{max}_{w}\\frac{|Xw|}{|w|} = \\mathrm{max}_{|w| = 1}\\left(|Xw|\\right).$$\n",
    "\n",
    "Попробуем осознать физический смысл этой штуки. Напомним, что строки матрицы $X$ — это координаты объектов $x_1,\\ldots,x_m$ в пространстве признаков. Произведение $Xw$ — это вектор из значений на тех же самых объектах некоторого нового признака, являющегося линейной комбинацией исходных с коэффициентами $w_1,\\ldots,w_n$:\n",
    "\n",
    "$$Xw = w_1\\begin{pmatrix} x_{11}\\\\ \\ldots \\\\ x_{m1}\n",
    "\\end{pmatrix} + w_2\\begin{pmatrix} x_{12}\\\\ \\ldots \\\\ x_{m2}\n",
    "\\end{pmatrix} + \\ldots + w_n\\begin{pmatrix} x_{1n}\\\\ \\ldots \\\\ x_{mn}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Соответственно, $|Xw|^2$ — это квадрат длины вектора, составленного из значений нового признака.\n",
    "\n",
    "Таким образом, первому сингулярному значению $\\sigma_1$ отвечает такой признак, у которого сумма квадратов значений максимальна, то есть признак, принимающий, условно говоря, самые большие значения.\n",
    "\n",
    "Резюмируя, мы можем сказать, что сингулярное разложение делает следующее:\n",
    "- находит новый признак (новое направление) вдоль которого \"дисперсия\" максимальна;\n",
    "- в ортогональной ему плоскости находит признак, вдоль которого \"дисперсия\" максимальна;\n",
    "- и так далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Технические детали (SVD в Питоне)**\n",
    "\n",
    "Есть несколько способов сделать в Питоне сингулярное разложение; мы пока предлагаем Вам использовать\n",
    "\n",
    "`import scipy.linalg as sla`\n",
    "\n",
    "`U, S, Vt = sla.svd(X, full_matrices=False)`\n",
    "\n",
    "Для ознакомления с особенностями этой функции рекомендуем обратиться к [документации](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html) (в частности, обратите внимание на то, какие именно объекты она возвращает)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поработать с какой-нибудь картинкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread(\"chain_small.jpg\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.mean(axis=2) # это матрица из интенсивностей серого цвета; её уже можно подвергать SVD\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем картинку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imgplot = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим сингулярное разложение этой матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "U, S, VT = svd(img, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Нарисуйте график диагональных элементов матрицы $\\Sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что они убывают достаточно быстро и есть надежда, что первые несколько компонент дадут картинку, близкую к исходной.\n",
    "\n",
    "**Важно:** при визуализации различных компонент в этом задании используйте только матричные операции. В частности, избегайте циклов, функций `map` и `reduce`, а также специальных функций, находящих суммы компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Визуализуйте первую компоненту ранга 1. Ожидали ли Вы увидеть именно это? Поясните."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# [✖‿✖]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Визуализуйте суммы компонент ранга 1 с первой по двадцатую, с первой по пятидесятую, с двадцатой по сотую, с двадцатой по последнюю. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ┌(ಠ_ಠ)┘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Как Вам кажется, какие компоненты нужно взять для достаточно хорошего восстановления исходного изображения? Аргументируйте свой ответ. Не забудьте визуализировать сумму выбранных компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Комментарий: для получения полного балла за это задания постарайтесь привести более убедительный аргумент, нежели Ваши субъективные впечатления от сравнения полученного изображения с исходным.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Ответ:__ Ваш ответ буквами прям в маркдауне:3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ( .-. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Во сколько раз меньше памяти (теоретически) потребуется для хранения нового изображения по сравнению с исходным?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Ответ:__ Ваш ответ буквами прям в маркдауне:3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Подсчитайте, сколько в действительности места в памяти компьютера занимают исходная матрица и компоненты её сингулярного разложения. Согласуется ли этот результат с ответом предыдущего пункта? Сделайте выводы.\n",
    "\n",
    "_Hint:_ достаточно сохранить да файла на компьютер в `.np` формате и сравнить их вес между собой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Ответ:__ Ваш ответ буквами прям в маркдауне:3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые из \"новых\" признаков — это признаки, значения которых, скажем так, наиболее разнообразны. Зачастую (хотя и не всегда) именно они несут в себе наиболее важные черты датасета. И если взять два-три первых, то датасет можно нарисовать и посмотреть на него — и, возможно, обнаружить какую-то структуру.\n",
    "\n",
    "С помощью функции `dsklearn.datasets.load_digits()` загрузите датасет рукописных цифр [MNIST](http://yann.lecun.com/exdb/mnist/). В нём есть несколько атрибутов; вам сейчас будут нужны `digits.data` (`np.array`, строки которого — это вытянутые в одну строку значения пикселей) и `digits.target` (в них записаны соответствующие цифры)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.target[0])\n",
    "\n",
    "plt.imshow(digits.data[0].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Примените к матрице `digits.data` сингулярное разложение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# (⌐■_■)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Визуализируйте данные, спроецировав их на такую плоскость, чтобы координаты точек соответствовали первым двум новым признакам. Не забудьте покрасить точки, отвечающие различным цифрам, в разные цвета (если Вы любите красивые визуализации, разберитесь, как вместо точек рисовать маленькие цифры)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# (❍ᴥ❍ʋ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Теперь вычтите из каждого признака его среднее значение, снова сделайте SVD и нарисуйте разноцветные точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# [̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Сравните выполненные Вами в двух предыдущих пунктах визуализации. Чем последняя выгодно отличается от первой?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#  ლ(ಠ益ಠლ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[2 балла (бонус)]__  Сравните работу SVD с другим методом понижения размерности: [случайными гауссовскими проекциями](http://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ヾ(๑╹◡╹)ﾉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Комментарий: Эксперименты без выводов, объясняющих полученные результаты, не оцениваются. Для получения полного балла за этот пункт постарайтесь провести как можно больше разноплановых экспериментов.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
